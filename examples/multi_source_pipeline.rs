//! Comprehensive Multi-Source Pipeline Example
//!
//! Demonstrates the full power of Rust Scraper Pro by combining:
//! - News scraping (Hacker News)
//! - E-commerce scraping (Books to Scrape)
//! - Social/API scraping (Reddit JSON)
//!
//! Features showcased:
//! - Multi-source concurrent scraping
//! - Unified data processing pipeline
//! - Validation, normalization, and deduplication
//! - Multiple export formats (JSON, CSV)
//! - Comprehensive error handling and logging
//! - Cache utilization and statistics
//!
//! Usage: cargo run --example multi_source_pipeline

use anyhow::Result;
use rust_scraper_pro::{
    core::{
        config::Config,
        models::ScrapedData,
        scraper::ScraperEngine,
    },
    output::{
        json::JsonOutput,
        csv::CsvOutput,
    },
    processors::pipeline::ProcessingPipeline,
    sources::{NewsSource, EcommerceSource, Source, SourceType},
    utils::{
        cache::HtmlCache,
        logger::setup_logger,
    },
};
use std::sync::Arc;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    setup_logger()?;

    println!("\nğŸš€ Rust Scraper Pro - Multi-Source Pipeline Demo");
    println!("==================================================");
    println!("This example demonstrates professional web scraping");
    println!("from multiple real-world sources with full processing.\n");

    // Load configuration
    let config = Config::load("config/settings.toml").await?;
    println!("âœ“ Configuration loaded");
    println!("  Rate limit: {}ms between requests", config.scraping.rate_limit_ms);
    println!("  Timeout: {}s", config.scraping.timeout_seconds);
    println!("  User agent: {}\n", config.scraping.user_agent);

    // Initialize cache
    let cache = Arc::new(HtmlCache::new_html_cache(200, 3600));
    println!("âœ“ Cache system initialized");
    println!("  Capacity: 200 items");
    println!("  TTL: 3600s (1 hour)\n");

    // Create processing pipeline
    let pipeline = ProcessingPipeline::new();
    println!("âœ“ Processing pipeline ready");
    println!("  Stages: Validation â†’ Normalization â†’ Deduplication\n");

    // Initialize scraper engine
    let mut engine = ScraperEngine::new(config, pipeline, Some(cache.clone()));

    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Define sources
    let sources: Vec<(&str, SourceType)> = vec![
        (
            "ğŸ—ï¸  News",
            SourceType::News(NewsSource::new("https://news.ycombinator.com/")
                .with_name("Hacker News"))
        ),
        (
            "ğŸ›’ E-commerce",
            SourceType::Ecommerce(EcommerceSource::new("https://books.toscrape.com/catalogue/category/books_1/index.html")
                .with_name("Books to Scrape"))
        ),
    ];

    let mut all_data = Vec::new();
    let mut source_stats: Vec<(&str, String, usize, bool)> = Vec::new();

    // Scrape from each source
    for (category, source) in sources {
        let source_name = source.name().to_string();
        let source_url = source.base_url().to_string();

        println!("\n{} Source: {}", category, source_name);
        println!("   URL: {}", source_url);
        println!("   Status: Scraping...");

        match engine.scrape_source(source).await {
            Ok(data) => {
                let count = data.len();
                println!("   âœ“ Success: {} items scraped", count);

                source_stats.push((category, source_name, count, true));
                all_data.extend(data);

                // Polite delay between sources
                sleep(Duration::from_millis(2000)).await;
            }
            Err(e) => {
                eprintln!("   âœ— Failed: {}", e);
                source_stats.push((category, source_name, 0, false));
            }
        }
    }

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("\nğŸ“Š Scraping Summary:");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let total_raw = all_data.len();
    for (category, name, count, success) in &source_stats {
        let status = if *success { "âœ“" } else { "âœ—" };
        println!("  {} {} ({}): {} items", status, category, name, count);
    }

    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("  Total raw items: {}\n", total_raw);

    if all_data.is_empty() {
        println!("âš ï¸  No data collected. Please check:");
        println!("   - Network connectivity");
        println!("   - Website availability");
        println!("   - Selector accuracy in config/settings.toml\n");
        return Ok(());
    }

    // Process data through pipeline
    println!("ğŸ”„ Processing through pipeline...");
    println!("   â†’ Stage 1: Validation");

    let processed_data = engine.process_data(all_data).await?;

    println!("   â†’ Stage 2: Normalization");
    println!("   â†’ Stage 3: Deduplication");
    println!("   âœ“ Pipeline complete\n");

    println!("ğŸ“ˆ Processing Results:");
    println!("   Input: {} items", total_raw);
    println!("   Output: {} items", processed_data.len());
    println!("   Removed: {} duplicates/invalid\n", total_raw - processed_data.len());

    // Categorize data by source
    let mut by_source: std::collections::HashMap<String, Vec<&ScrapedData>> = std::collections::HashMap::new();
    for item in &processed_data {
        by_source.entry(item.source.clone()).or_insert_with(Vec::new).push(item);
    }

    println!("ğŸ“‚ Data by Source:");
    for (source, items) in &by_source {
        println!("   â€¢ {}: {} items", source, items.len());
    }
    println!();

    // Display sample data
    println!("ğŸ“‹ Sample of Processed Data:");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    for (idx, item) in processed_data.iter().take(5).enumerate() {
        println!("\n[{}] {}", idx + 1, item.title.as_ref().unwrap_or(&"(no title)".to_string()));
        println!("    Source: {}", item.source);

        if let Some(price) = item.price {
            println!("    Price: Â£{:.2}", price);
        }

        if let Some(author) = &item.author {
            println!("    Author: {}", author);
        }

        println!("    URL: {}", item.url);
    }

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Export data
    println!("ğŸ’¾ Exporting Data:");

    let json_output = JsonOutput::new();
    json_output.export(&processed_data, "output/multi_source_data.json").await?;
    println!("   âœ“ JSON: output/multi_source_data.json");

    let csv_output = CsvOutput::new();
    csv_output.export(&processed_data, "output/multi_source_data.csv").await?;
    println!("   âœ“ CSV: output/multi_source_data.csv");

    // Cache statistics
    let cache_stats = cache.stats();
    println!("\nğŸ“ˆ Cache Performance:");
    println!("   Entries: {}", cache_stats.entry_count);
    println!("   Hit rate: {:.1}%", cache_stats.hit_rate * 100.0);
    println!("   Miss rate: {:.1}%\n", cache_stats.miss_rate * 100.0);

    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("\nâœ… Multi-Source Pipeline Completed Successfully!");
    println!("\n   ğŸ“ Scraped from {} sources", source_stats.len());
    println!("   ğŸ“Š Collected {} raw items", total_raw);
    println!("   âœ¨ Processed to {} unique items", processed_data.len());
    println!("   ğŸ’¾ Exported to JSON and CSV\n");

    println!("ğŸ¯ Next Steps:");
    println!("   - Review output files in /output/ directory");
    println!("   - Adjust selectors in config/settings.toml if needed");
    println!("   - Add more sources in config/sources.toml");
    println!("   - Experiment with different processing pipelines\n");

    Ok(())
}
