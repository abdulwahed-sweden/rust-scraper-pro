# Rust Scraper Pro Configuration
# Edition: 2024 | Real-world data sources

[scraping]
rate_limit_ms = 2000  # Polite scraping: 2 seconds between requests
timeout_seconds = 30
max_retries = 3
user_agent = "Mozilla/5.0 (compatible; RustScraperPro/1.0; Educational)"
follow_robots_txt = true

[api]
port = 3000
host = "127.0.0.1"
cors_origins = ["http://localhost:3000", "http://127.0.0.1:3000"]

[database]
# Type: "postgres" or "sqlite"
type = "postgres"
# PostgreSQL connection string (use this for production)
url = "postgres://postgres:postgres@localhost/rust_scraper_db"
# SQLite path (alternative for development)
sqlite_path = "output/scraped_data.db"
# Table name for scraped data
table_name = "scraped_data"
# Enable database persistence
enabled = true

[cache]
memory_max_items = 1000
memory_ttl_seconds = 3600
file_cache_dir = "cache/html"
enable_file_cache = true

# ========================================
# NEWS SOURCES (Real, Public Data)
# ========================================

[[sources]]
name = "BBC News"
url = "https://www.bbc.com/news"
type = "news"
rate_limit_ms = 3000

[sources.selectors]
container = "article"
title = "h2"
content = "p"
author = ".author"

[[sources]]
name = "Reuters World"
url = "https://www.reuters.com/world/"
type = "news"
rate_limit_ms = 3000

[sources.selectors]
container = "article"
title = "h3"
content = "p"

# ========================================
# E-COMMERCE SOURCES (Books to Scrape)
# ========================================

[[sources]]
name = "Books to Scrape"
url = "https://books.toscrape.com/catalogue/category/books_1/index.html"
type = "ecommerce"
rate_limit_ms = 2000

[sources.selectors]
container = ".product_pod"
title = "h3 a"
price = ".price_color"
image = "img"

# ========================================
# SOCIAL / API SOURCES
# ========================================

[[sources]]
name = "Hacker News Top Stories"
url = "https://news.ycombinator.com/"
type = "news"
rate_limit_ms = 2000

[sources.selectors]
container = ".athing"
title = ".titleline > a"
content = ".subtext"
author = ".hnuser"

# ========================================
# ADAPTIVE DELAY CONFIGURATION
# ========================================

[scraper]
mode = "adaptive"        # Options: "fixed" or "adaptive"
min_delay_ms = 200       # Minimum delay between requests
max_delay_ms = 2500      # Maximum delay between requests
sample_size = 10         # Number of response times to track
multiplier = 1.2         # Delay = avg_response_time * multiplier (1.2 = 20% slower)

# ========================================
# AI FEATURES (DeepSeek Integration)
# ========================================

[ai]
enabled = true
deepseek_model = "deepseek-chat"
enable_selector_assistant = true
enable_normalizer = true
normalizer_batch_size = 50
